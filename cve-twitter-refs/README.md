# README

I'm worried that Twitter's impending collapse is going to nuke a lot of CVE
references. Let's fix that.

## Collecting CVEs with Twitter references

First, let's get the list of CVEs that have Twitter references. That's easy
and fun on the NVD's website:

`curl --location "https://services.nvd.nist.gov/rest/json/cves/2.0?keywordSearch=twitter.com%2F" > cves.json`

Format that thing with `jq` so I can actually read it:

`jq -M < cves.json > cves-jq.json && mv cves-jq.json cves.json`

Get the list of Twitter references:

`cat cves.json| jq '.vulnerabilities[].cve.references[] | select(.url|test("http.*twitter.com/")).url'`

Hmm, seems like a lot, and also there are just some usernames in there, no posts. How about:

`cve-twitter-refs % cat cves.json| jq '.vulnerabilities[].cve.references[] | select(.url|test("http.*twitter.com/.*/status")).url' > urls.txt`

Better, but mine eyes see dupes. So:

```bash
cat cves.json| jq '.vulnerabilities[].cve.references[] | select(.url|test("http.*twitter.com/.*/status")).url' > \
sort | uniq > urls.txt
```

377 references total. That's quite tractable. Oh, look at that:

http://twitter.com/dakami/statuses/7104238406

That's a poignant Tweet we'll definitely want to save.

## Ensuring these references are saved off

So now we can admire all these tweets that are Real and Official References to CVEs, but what I really want to do is to make sure they're accessible for future generations of software archeologists. Naturally, my first instinct is to look at the Internet Archive's API, so just to test real quick:

```bash
curl --location "http://archive.org/wayback/available?url=twitter.com/dakami/statuses/7104238406"

{"url": "twitter.com/dakami/statuses/7104238406", "archived_snapshots": {"closest": {"status": "200", "available": true, "url": "http://web.archive.org/web/20211206000758/https://twitter.com/dakami/statuses/7104238406", "timestamp": "20211206000758"}}}

```

That worked... eventually. After a couple 502 Bad Gateway / 504 Gateway Time-Out errors and retries. Not ideal. Maybe someone else has already written something a little more robust. But, I've now noticed another snag -- I'm too late. There's a lot of Twitter references that are already gone. For example:

https://twitter.com/tippingpoint1/status/1351635812 : Suspended account, I wonder what they did. :)
https://twitter.com/spendergrsec/status/4916661870 : Deleted tweet, which isn't surprising.

But, I have a couple examples of bad links, and one example of a good link, so I should be able to suss out the difference before I run around and archive everything. Sadly, I lost my Twitter API keys long ago, and not interested in setting up new ones. It's only a few hundred tweets, though, so maybe manual is the best approach here?

...assuming something wonderful happens here...

Let's talk to my pal Erick, he's good at this sort of thing.

## Archive Now

Okay, we're in a place now where I can actually start archiving things, thanks to [Archive Now](https://github.com/oduwsdl/archivenow). So let's follow their docs and get that set up.

Since Internet Archive is plagued with timeouts, let's for now just concentrate on saving off these Tweets as WARC files, and optionally stash them on 

# README

[I](https://infosec.exchange/@todb) am worried that Twitter's impending
collapse is going to nuke a lot of CVE references. Let's fix that.

| File                           | Description   |
| --------------                 | -----------   |
| [cves.json](./cves.json)       | The collection of all 420 (nice) CVEs with Twitter references |
| [urls.txt](./url)              | Just the URLs of those references |
| [good.txt](./good.txt)         | Known good URLs that still work as references today |
| [bad.txt](./bad.txt)           | Known bad URLs -- deleted, private, suspended, etc |
| [archives.csv](./archives.csv) | Archived good twitter links |

# Want to help?

Go nuts. Don't ask for permission. Just figure out how to save off these CVE
references somewhere other than Twitter for a start, then tell people about
it.

Eventually, we'll want to figure out how to archive *every* CVE reference, in
a couple different places, and push back against the linkrot that afflicts the
CVE list.

What follows is a journal/log of what I've tried so far.

## Collecting CVEs with Twitter references

First, let's get the list of CVEs that have Twitter references. That's easy
and fun on the NVD's website:

`curl --location
"https://services.nvd.nist.gov/rest/json/cves/2.0?keywordSearch=twitter.com%2F"
> cves.json`

Format that thing with `jq` so I can actually read it:

`jq -M < cves.json > cves-jq.json && mv cves-jq.json cves.json`

Get the list of Twitter references:

`cat cves.json| jq '.vulnerabilities[].cve.references[] |
select(.url|test("http.*twitter.com/")).url'`

Hmm, seems like a lot, and also there are just some usernames in there, no
posts. How about:

`cve-twitter-refs % cat cves.json| jq '.vulnerabilities[].cve.references[] |
select(.url|test("http.*twitter.com/.*/status")).url' > urls.txt`

Better, but mine eyes see dupes. So:

```bash
cat cves.json | jq '.vulnerabilities[].cve.references[] | select(.url|test("http.*twitter.com/.*/status")).url' | sort | uniq > urls.txt
```

377 references total. That's quite tractable. Oh, look at that:

http://twitter.com/dakami/statuses/7104238406

That's a poignant Tweet we'll definitely want to save.

## Ensuring these references are saved off

So now we can admire all these tweets that are Real and Official References to
CVEs, but what I really want to do is to make sure they're accessible for
future generations of software archeologists. Naturally, my first instinct is
to look at the Internet Archive's API, so just to test real quick:

`curl --location
"http://archive.org/wayback/available?url=twitter.com/dakami/statuses/7104238406"`

results in

```json
{"url": "twitter.com/dakami/statuses/7104238406", "archived_snapshots": {"closest": {"status": "200", "available": true, "url": "http://web.archive.org/web/20211206000758/https://twitter.com/dakami/statuses/7104238406", "timestamp": "20211206000758"}}}
```

...eventually. After a couple 502 Bad Gateway / 504 Gateway Time-Out errors
and retries. Not ideal. Maybe someone else has already written something a
little more robust. But, I've now noticed another snag -- I'm too late.
There's a lot of Twitter references that are already gone. For example:

https://twitter.com/tippingpoint1/status/1351635812 : Suspended account, I
wonder what they did. :) https://twitter.com/spendergrsec/status/4916661870 :
Deleted tweet, which isn't surprising.
https://twitter.com/__Obzy__/status/864704956116254720 | Private tweet, so
effectively offline now.

### Original redirects

Oh, and a lot of the tweet references are behind layers of redirects, so what
gets archived is not *exactly* the URL that the CVE reference uses. This means
we'll have to keep track of what we archive and keep that pointer (and
eventually, update the CVEs with the new archived reference). Here's an
example:

http://twitter.com/d_gianni/statuses/562628862648270849/photo/1 (referenced at
[CVE-2014-6301](https://nvd.nist.gov/vuln/detail/CVE-2014-6301) redirects to
https://twitter.com/d_gianni/status/562628862648270849 (the lack of the
'photo/1'), and is now archived at https://archive.ph/2as1J. But, if you look
for that original 'photo/1' link on https://archive.ph, you're not going to
find it, and you might assume it's just gone forever. That seems like a
problem. The photo/1 thing is relatively easy to spot as a redirect, but did
you know it's possible to change Twitter usernames and then have *those*
redirect? Yep, check it out:
[finnwea](https://twitter.com/finnwea/status/965279233030393856) is now
[tijme](https://twitter.com/tijme/status/965279233030393856). Wild stuff.

But, all that said, I have a three examples of bad links, and one example of a
good link, so I should be able to suss out the difference before I run around
and archive everything. Sadly, I lost my Twitter API keys long ago, and not
interested in setting up new ones. It's only a few hundred tweets, though, so
maybe manual is the best approach here? Surely there's something...

## Archive Now

Okay, after trying a few (and failing), I've hit on [Archive
Now](https://github.com/oduwsdl/archivenow). So let's follow their docs and
get that set up.

Since Internet Archive is plagued with timeouts, let's for now just
concentrate on saving off these Tweets as WARC files, and optionally stash
them on https://archive.is which is actually https://archive.ph also known as
[archive.today](https://archive.today). They seems responsive and peppy. We'll
also want to save a local WARC file because why not. So, a shell script like
this should do the trick:

```
cat good.txt | while read i;
do
  FNAME=`echo $i | cut -d "/" -f 4,6 | sed "s#\/#-#" | tr -d '"'` ;
  echo Archiving "$i" and saving "$FNAME.warc"; 
  archivenow --is $i --warc ./warc/$FNAME | tee -a log.txt ;
  unset FNAME;
done
```

(The above assumes you have a local directory, ./warc/, to write to.)

Alas, this doesn't really work.

### Too Many Redirects

```plain
Error (The Archive.is): Exceeded 30 redirects.
./warc/#!-status.warc
Error (The Archive.is): Exceeded 30 redirects.
./warc/ASUSUSA-357612236392509440.warc
Error (The Archive.is): Exceeded 30 redirects.
./warc/Laughing_Mantis-633839231840841728.warc
Error (The Archive.is): Exceeded 30 redirects.
./warc/Laughing_Mantis-633839771865886721.warc
Error (The Archive.is): Exceeded 30 redirects.
./warc/S9Labs-519576582742999043.warc
Error (The Archive.is): Exceeded 30 redirects.
./warc/WisecWisec-17254776077.warc
Error (The Archive.is): Exceeded 30 redirects.
./warc/agl__-437029812046422016.warc
```

Well, darn. Looks like I hit this redirect complaint business a lot. It's not
the original redirect problem described above, it's just how Twitter pages
work.

## Manual archiving

Manually archiving does seem to work reliably at archive.is, but this client
and [archivetoday](https://www.npmjs.com/package/archivetoday) (a node client)
both fail out, consistently, since there's such a huge boatload of links to
follow on every tweet.

At least it looks like I can save locally with archivenow. These WARC files
seem difficult to parse, and don't seem to actually collect the Tweet bodies,
so not particularly useful? It's still only 277 "good" tweets today.

I manually archived a few and saved those off in a CSV, and the [Chrome
Plugin](https://chrome.google.com/webstore/detail/archive-page/gcaimhkfmliahedmeklebabdgagipbia?hl=en-US)
seems to work the best of all the tools I explored tonight.

It would be fine if people just bashed that button, but I'm still worried I'm
in a race against people going private and deleting their history, not to
mention the seemingly inevitable collapse of Twitter.

### Button Bashing

Well, that Chrome plugin is working out well. Couple small issues:
* Sometimes, I get hit with a CAPTCHA, so that kicks out easy automation by
  keystroke repeating.
* Sometimes, the archival request triggers a "something went wrong" error on
  Twitter, so have to eyeball those.
* When someone else has already archived a tweet, it goes faster, but also
  drops an extra click in.

## Done, for now?

Okay, so I've archived all these tweets over on
[archivetoday](https://www.npmjs.com/package/archivetoday). Which basically
means I've just spent a couple hours making sure some security info of
questionable value is backed up from one website to another, much smaller,
website. Obviously, this isn't a particularly tenable solution, but it's 100%
better than how things were about 30 hours ago, when nearly none of these
tweets were archived at all.

## TODOs

* Automate archiving of CVE references when they first show up in a reliable,
  redundant way. Talk to the [CVE Automation Working
  Group](https://www.cve.org/ProgramOrganization/WorkingGroups#AutomationWorkingGroupAWG)
  about this. Or [NVD](https://nvd.nist.gov/) since they're already kind of
  archivists for CVE.
  * This could pretty easily be *anyone's* job, since references are public
    and new CVEs are published in known places.
  * The most expensive/valuable CVEs should be *closely* monitored for dead
    links. I'm looking at you,
    [KEV](https://www.cisa.gov/known-exploited-vulnerabilities-catalog)
* Figure out how to point to these archived references in the CVE records
  themselves.
  * These are low-fidelity copies. While Twitter remains, these should be just
    backups.
  * People will certainly delete/privatize more tweets.
* You can't trust that I haven't screwed up some data entry. Figure out how to
  automatically verify that, somehow.
* While Twitter's disintegration is the immediate concern, this should be done
  for all references. Two is one and one is none, and all that.

# Thanks

Thanks to those people who helped with code or manual labor:

* [@todb@infosec.exchange](https://infosec.exchange/todb)
* [@ErickGalinkin@mastodon.social](https://mastodon.social/@erickgalinkin)
* [@level@mastodon.sdf.org](https://mastodon.sdf.org/@level)